### xor

```r
xor <- function(x, y) { (x | y) & !(x & y) }
```

Annotated

```r
xor <- function(x:lgl[]@mutate, y:lgl[]@borrow) -> lgl[]@fresh {
    _1:lgl[]@fresh = `|`(x, y)
    _2:lgl[]@fresh = `&`(x, y)    {@mutates x}
    _3:lgl[]@fresh = `!`(_2)      {@mutates _2}
    _4:lgl[]@fresh = `&`(_1, _3)  {@mutates _1}
    return _4
}

xor <- function(x:lgl[]@borrow, y:lgl[]@mutate) -> lgl[]@fresh {
    _1:lgl[]@fresh = `|`(x, y)
    _2:lgl[]@fresh = `&`(x, y)    {@mutates y}
    _3:lgl[]@fresh = `!`(_2)      {@mutates _2}
    _4:lgl[]@fresh = `&`(_1, _3)  {@mutates _1}
    return _4
}

xor <- function(x:lgl[]@borrow, y:lgl[]@borrow) -> lgl[]@fresh {
    _1:lgl[]@fresh = `|`(x, y)
    _2:lgl[]@fresh = `&`(x, y)
    _3:lgl[]@fresh = `!`(_2)      {@mutates _2}
    _4:lgl[]@fresh = `&`(_1, _3)  {@mutates _1}
    return _4
}

xor <- function(x:lgl, y:lgl) -> lgl {
    _1:lgl = `|`(x, y)
    _2:lgl = `&`(x, y)
    _3:lgl = `!`(_2)
    _4:lgl = `&`(_1, _3)
    return _4
}
```

Notes

- There are two ways we can compile `xor` to consume vectors, suggesting that we compile `@mutate` based on callee (compile context says "this argument is fresh", then if the compiled closure actually mutates the argument, it gets the `@mutate` annotation).
- Scalars are never mutated in-place, because even if they're wrapped in SEXPs, we should probably use global constants, lazily interning if necessary.
- In the syntax, `` `&` `` and `` `|` `` are overloaded based on the argument types: a function with the same name applied to arguments of different types is actually a different function. We could make the specializations explicit with syntax like `` `&`(:lgl[]@mutate, :lgl[]@borrow) ``, but I think the added information is too obvious. I chose to only annotate local variable types because I think they can be less obvious particularly with large chains of statements.

### isTRUE

```r
isTRUE <- function(x) is.logical(x) && length(x) == 1L && !is.na(x) && x
```

Annotated

```r
isTRUE <- function(x:bool) -> bool {
    return x
}

isTRUE <- function(x:lgl) -> bool {
    _1:bool <- is.na(x)
    if _1 goto ^1 else ^2
  ^1:
    return FALSE
  ^2:
    assume x is bool
    return x
}

isTRUE <- function(x:any@!reflect@borrow) -> bool+attrs {
    _1:bool <- is.logical(x)
    if _1 goto ^1 else ^2
  ^1:
    assume x is lgl+attrs[]
    _2:whole|real <- length(x)
    _3:bool <- `==`(_2, 1L)
    if _3 goto ^3 else ^2
  ^2:
    return FALSE
  ^3:
    assume x is lgl+attrs
    _4:bool <- is.na(x)
    if _4 goto ^2 else ^4
  ^4:
    assume x is bool+attrs
    return x
}
```

Notes

- Can't prove `isTRUE(:lgl)` returns a boolean, because `` `&&`(TRUE, NA) `` produces `NA`, and it's hard for the compiler to determine that `_2` is `FALSE` if `x` is `NA`. But we can speculate.
  - Since the speculation only affects the return type, should we actually do it, or defer it to the callees that will only do it when it matters? The former can waste overhead on a useless speculation. But the latter increases code size; moreover, it decreases understandability, since you may have a lot of functions whose return types are less specific than they could be, but they always get speculated whenever the more specific type is necessary. Ultimately, I chose the former (a speculation is not considered redundant when it affects the return type), assuming the overhead of speculating is negligable enough to justify the code-size/readability improvements.

- Alternatively or additionally, the speculated **assumptions** can be replaced with **inferences** statically proven by the conditions. Functions like `is.logical`, `is.na` etc. can be hard-coded, and user-defined functions can be inferred, with annotations that state, when they return a specific value, their argument(s) have a **guarantee**. We can chain these guarantees, e.g. if a variable that must be a specific value for a particular guarantee, is compared with that value, the comparison returning `TRUE` provides that guarantee; and negation/conjunction/disjunction affects guarantees. We also hard-code and infer annotations that provide guarantees when a returned value is of a particular type, which lets us build off existing speculations (if we determine that we want to speculate the returned value is said type, suddenly we have a lot more info).

With guarantees:

```r
isTRUE <- function(x:lgl) -> bool @guarantee(TRUE => x is bool) {
    _1:bool <- is.na(x)         {@guarantee(TRUE => x is bool)}
    if _1 goto ^1 else ^2
  ^1:
    return FALSE
  ^2:
    infer x is bool
    return x
}

isTRUE <- function(x:any@!reflect) -> bool+attrs @guarantee(TRUE => x is bool+attrs) {
    _1:bool <- is.logical(x)    {@guarantee(TRUE => x is lgl+attrs[])}
    if _1 goto ^1 else ^2
  ^1:
    infer x is lgl+attrs[]
    _2:whole|real <- length(x)  {@guarantee(1L => x is lgl+attrs)}
    _3:bool <- `==`(_2, 1L)     {@guarantee(TRUE => _2 == 1L)} ==> {@guarantee(TRUE => x is lgl+attrs)}
    if _3 goto ^3 else ^2
  ^2:
    return FALSE
  ^3:
    infer x is lgl+attrs
    _4:bool <- is.na(x)         {@guarantee(FALSE => x is bool+attrs)}
    if _4 goto ^2 else ^4
  ^4:
    infer x is bool+attrs
    return x
}
```

- The problem with guarantees is that they can create complex constraints that may be hard/impossible to automatically satisfy. On the other hand, assumptions can "prove" *any* true property, by not actually proving, but inserting a guard at runtime which will deoptimize when false. If the cost of these guards is negligable, it doesn't seem worth the effort to add new intrinsics and an SAT solver to the compiler for some of them to be elided.

### append

```r
append <- function(x, values, after = length(x)) {
    lengx <- length(x)
    if (!after) c(values, x)
    else if (after >= lengx) c(x, values)
    else c(x[1L:after], values, x[(after + 1L):lengx])
}
```

Annotated

```r
append <- function[T <: !obj](x:T[]@borrow, values:T[]@borrow) -> T[]@fresh {
    _1:T[] = c(x, values)
    return _1
}

append <- function[T <: !obj](x:T[]@borrow, values:T[]@borrow, after:int = 0) -> T[]@fresh {
    _1:T[] = c(values, x)
    return _1
}

append <- function[T <: !obj](x:T[]@borrow, values:T[]@borrow, after:whole) ->{error} T[]@fresh {
    lengx:whole = length(x)
    _1:bool = `>=`(after, lengx)
    if _1 goto ^1 else ^2
  ^1:
    _2:T[]@fresh = c(x, values)
    return _2
  ^2:
    _3:whole[]@range = `:`(1L, after)
    _4:T[] = `[`(x, _3)                {error}
    _5:whole = `+`(after, 1L)
    _6:whole[]@range = `:`(_5, lengx)
    _7:T[] = `[`(x, _6)                {error}
    _8:T[]@fresh = c(_4, values, _7)
    return _8
}

append <- function[T <: !obj](x:T[]@borrow, values:T[]@borrow, after:nat) ->{error} T[]@fresh {
    lengx:whole = length(x)
    _1:bool = `!`(after)
    if _1 goto ^1 else ^2
  ^1:
    _2:T[]@fresh = c(values, x)
    return _2
  ^2:
    _3:bool = `>=`(after, lengx)
    if _3 goto ^3 else ^4
  ^3:
    _4:T[]@fresh = c(x, values)
    return _4
  ^4:
    _5:nat[]@range = `:`(1L, after)
    _6:T[] = `[`(x, _5)                {error}
    _7:whole|real = `+`(after, 1L)
    _8:whole|real[]@range = `:`(_7, lengx)
    _9:T[] = `[`(x, _8)                {error}
    _10:T[]@fresh = c(_6, values, _9)
    return _9
}

append <- function[T <: !obj](x:T[]@borrow, values:T[]@borrow, after:int|NA) ->{error} T[]@fresh {
    lengx:whole = length(x)
    _1:lgl = `!`(after)
    assume _1 is bool
    if _1 goto ^1 else ^2
  ^1:
    _2:T[]@fresh = c(values, x)
    return _2
  ^2:
    _3:lgl = `>=`(after, lengx)
    assume _3 is bool
    if _3 goto ^3 else ^4
  ^3:
    _4:T[]@fresh = c(x, values)
    return _4
  ^4:
    _5:int[]@range = `:`(1L, after)  {error}
    _6:T[] = `[`(x, _5)              {error}
    _7:num|NA = `+`(after, 1L)
    _8:num[]@range = `:`(_7, lengx)  {error}
    _9:T[] = `[`(x, _8)              {error}
    _10:T[]@fresh = c(_6, values, _9)
    return _9
}
```

Notes

- We should compile versions of functions where default arguments are not provided (so the compiler knows they are exact and can constant-fold etc.), specifically when the callee doesn't provide those arguments.
  - We should also record arguments and variables being the same value many times, then compile versions where the arguments are *usually* the same value, and speculate on variables that are *always* the same value.
- We can constant-fold expressions like `âˆ€x:!obj[]. length(x) >= length(x)` by adding a GVN/CSE/e-graph analysis that looks for functions without "read-outside-state" effects that have equivalent inputs, produces an equivalent output.
- I chose to make growing/shrinking a vector not mutate its argument, but always copy. The reason being, usually we have to re-allocate and move the data anyways. There are some cases where we don't and furthermore, not doing so is very expensive: e.g. `foo <- c(); for (i in 1:100) foo <- c(foo, i)`. *However*, those cases are handled by R's runtime copy-on-write semantics (which we probably have to preserve, because there are other cases that would be expensive where we probably can't prove last-use but are handled by the runtime).
- Some edge-cases are pervasive. For example, we can't disprove subsetting causes an out-of-bounds error. We also can't disprove addition causes a to-real conversion because of integer overflow.
  - I probably missed some other examples of these cases, here and in other examples...

### mode

```r
mode <- function(x) {
    if(is.expression(x)) return("expression")
    if(is.call(x))
        return(switch(deparse(x[[1L]])[1L],
                      "(" = "(",
                      ## otherwise
                      "call"))
    if(is.name(x)) "name" else
    switch(tx <- typeof(x),
           double =, integer = "numeric", # 'real=' dropped, 2000/Jan/14
           closure =, builtin =, special = "function",
           ## otherwise
           tx)
}
```

Annotated

```r
mode <- function(x:any@!reflect) -> str {
    _1:bool = is.expression(x)
    if _1 goto ^1 else ^2
  ^1:
    return "expression"
  ^2:
    _2:bool = is.call(x)
    if _2 goto ^3 else ^4
  ^3:
    assume x is lang
    _3:sym|lang = `[[`(x, 1L)
    _4:str[] = deparse(_3)
    _5:str = `[`(_4, 1L)
    _6:bool = `==`(_5, "(")
    if _6 goto ^5 else ^6
  ^4:
    _7:bool = is.name(x)
    if _7 goto ^7 else ^8
  ^5:
    return "expression"
  ^6:
    return "call"
  ^7:
    return "name"
  ^8:
    tx:str = typeof(x)
    _8:bool = `==`(tx, "double")
    if _8 goto ^9 else ^10
  ^9:
    return "numeric"
  ^10:
    _9:bool = `==`(tx, "integer")
    if _9 goto ^9 else ^11
  ^11:
    _10:bool = `==`(tx, "closure")
    if _10 goto ^12 else ^13
  ^12:
    return "function"
  ^13:
    _11:bool = `==`(tx, "builtin")
    if _11 goto ^12 else ^14
  ^14:
    _12:bool = `==`(tx, "special")
    if _12 goto ^12 else ^15
  ^15:
    return tx
}
```

### anyDots

```r
anyDots <- function(args) {
    for (i in 1:length(args)) {
        a <-args[[i]]
        if (!missing(a) && identical(a, quote(`...`)))
            return(TRUE)
    }
    return(FALSE)
}
```

Annotated

```r
anyDots <- function(args:sym|miss[]@borrow) ->{error} bool {
    _forLast:whole = length(args)
    i:nat = 0L
    goto ^forStep
  ^forStep(i.1(i:^entry, i.2:^forBody, i.2:^1)):
    i.2:whole = `+`(i.1, 1L)
    _breakFor:bool = `<`(_forLast, i.2)
    if _breakFor goto ^forExit else ^forBody
  ^forExit:
    return FALSE
  ^forBody:
    a:sym|miss = `[[`(args, i.2)  {error}
    _2:bool = missing(a)
    if _2 goto ^forStep else ^1
  ^1:
    _3:bool = identical(a, '...)
    if _3 goto ^2 else ^forStep
  ^2:
    return TRUE
}
```

Notes

- Someone mentioned before, but the functions in `compiler/R/cmp.R` are the best to test the compiler (both whether it works and whether it produces speedups).
- We should have explicit optimization for `1L:â€¦` and `1:â€¦` ranges, especially those where the start is a real but we can convert to an integer.

# missingArgs

```r
missingArgs <- function(args) {
    val <- logical(length(args))
    for (i in seq_along(args)) {
        a <- args[[i]]
        if (missing(a))
            val[i] <- TRUE
        else
            val[i] <- FALSE
    }
    val
}
```

Annotated

```r
missingArgs <- function(args:sym|miss[]@borrow) ->{error} lgl[]@fresh {
    _1:whole = length(args)
    val:bool[]@fresh = logical(_1)
    _forLast:whole <- length(val)
    i:nat = 0L
    goto ^forStep
  ^forStep(i.1(i:^entry, i.2:^forBody), val.1(val:^entry, val.2:^forBody)):
    i.2:whole = `+`(i.1, 1L)
    _breakFor:bool <- `<`(_forLast, i.2)
    if _breakFor goto ^forExit else ^forBody
  ^forExit:
    return val.1
  ^forBody:
    a:sym|miss <- `[[`(args, i.2)  {error}
    _2:bool <- missing(a)
    val.2:bool[]@fresh <- `[`(val.1, i.2, _2)   {@mutates val.1}
    goto ^forStep
}
```

Notes

- Like above, we should have an optimization for `seq_along` for loops. One thing we could do is use an analysis to rewrite `seq_along(x)` to/from `1L:length(x)`.
  - e-graphs seem to be very useful, because there are a lot of these rewrite rules. For example, we should also be able to "normalize" equivalent expressions, like convert `identical(<scalar1>, <scalar2>)` into `` `==`(<scalar1>, <scalar2>) ``, since they should ultimately be optimized the same.
- In this case, I assume we can't infer `length(val) == length(args)`. However, we could if explicit support was added for `length(logical(n)) ==> n`. I don't think the performance gain of this particular optimization is worth the complexity though, so it would probably only exist if it's part of a more general optimization.
- I assume we *can* optimize the redundant `if (missing(a))` though.

### patchLabels

```r
patchlabels <- function(cntxt) {
    offset <- function(lbl) {
        if (is.null(labels[[lbl]]))
            cntxt$stop(gettextf("no offset recorded for label \"%s\"", lbl),
                        cntxt)
        labels[[lbl]]
    }
    for (i in 1 : codeCount) {
        v <- codeBuf[[i]]
        if (is.character(v))
            codeBuf[[i]] <<- offset(v)
        else if (typeof(v) == "list") {
            off <- as.integer(lapply(v, offset))
            ci <- putconst(off)
            codeBuf[[i]] <<- ci
        }
    }
}
```

Annotated

```r
patchlabels <- function(cntxt:env) {
    labels:any = ld('labels)@?
    codeBuf:any = ld('codeBuf)@?
    codeCount:any = ld('codeCount)@?
    putConst:any = ld('putConst)@?
    assume labels is int|nil[]
    assume codeBuf is val[]
    assume codeCount is nat
    assume putConst(:int[]) is int
    offset = function(lbl:str) -> int {
        _1:int|nil = `[[`(labels, lbl)
        _2:bool = is.null(_1)
        if _2 goto ^1 else ^2
      ^1:
        stop:any = `$`(cntxt, 'stop)
        assume stop is fun
        _3:str = gettextf("no offset recorded for label \"%s\"", lbl)
        _ = stop(_3, cntxt)
        goto ^2
      ^2:
        assume _1 is int
        return _1
    }
    i:nat = 0L
    goto ^forStep
  ^forStep(i.1(i:^entry, i.2:^2, i.2:^4), codeBuf.1(codeBuf:^entry, codeBuf.1:^2, codeBuf.2:^4)):
    i.2:whole = `+`(i, 1L)
    _breakFor:bool = `<`(codeCount, i.2)
    if _breakFor goto ^forExit else ^forBody
  ^forBody:
    v:val = `[[`(codeBuf, i.2)
    _1:bool = is.character(v)
    if _1 goto ^1 else ^2
  ^1:
    assume v is str
    _2:int = offset(v)
    goto ^4
  ^2:
    _3:str = typeof(v)
    _4:bool = `==`(_3, "list")
    if _4 goto ^3 else ^forStep
  ^3:
    assume v is str[]
    off:int[]@fresh = sapply(v, offset)
    ci:int = putconst(off)
    goto ^4
  ^4(_5(_2:^1, ci:^3)):
    codeBuf.2:val[]@fresh = `[[<-`(codeBuf.1, i.2, _5)
    goto ^forStep
  ^forExit:
    st('codeBuf, codeBuf.1)@?
    return NULL
}
```

Notes

- If we can disprove reflection, we can hoist loading external variables to the beginning and push storing them to the end, which lets us not worry about environments. One thing we can do is speculate that the loaded variables aren't lazy promises before.
    - I don't know if it's possible, but it would be nice to also have inner closures reference locals from outer closures. It would require an analysis to prove the closure doesn't escape and some runtime technique, but it would remove environments even in this example.
- Function speculation: we don't need to assume `putConst` is a function that takes and returns an integer, we specifically need to assume that `putConst` applied with an integer returns `int`. The distinction is important because `putConst` may have other overloads.
- We can use e-graphs or some other form of rewriting to replace `$` with `[`.
- Syntax can be confusing, because `stop` here refers to the local `stop`, if global `stop` was used it would have to be something like `base:::stop`. This is why I originally prefixed variables with `%`. However, it may be workable, because the lack of `%`s means the syntax is a lot cleaner in other areas, and moreover, it's probably very rare that a shadowed builtin is called (and when it is, it's probably almost always qualified in the same way we'd qualify it in the IR).
- The compiler should be able to merge the `labels[[lbl]]` loads, and it may be able to also merge the `codeBuf[[i]] <<-` stores. For the example I assumed both.
- I assume we have polymorphic type annotations on `lapply`. However, we may not even need them (or polymorphic effect annotations), because alternatively, we could specialize `lapply` for `int` and some other functions.
  - I don't know if we can compile annotations `lapply` directly because it's very complicated (it was going to be one of the examples before I realized that), but we can manually add annotations to built-ins like it.
  - I also assume we can simplify `as.integer(lapply(â€¦))` into `sapply(â€¦)` (they're equivalent if the function returns an integer, at least excluding any edge cases I didn't realize).
- We *could* optimize the mutation of `codeBuf[[i]] <-` to be in-place unless if we do loop unrolling, although I didn't do any in the examples. However, I still don't know if we can optimize every assignment in for loops in the general case.

---

General notes

- In every example, I assume we can elide environments and objects everywhere. If not, it makes the code a lot harder to optimize, because local variables are constantly loaded and stored, and functions return `any` since they could dispatch. But in all these examples I think we can elide environments practically everywhere.
- I also assume things like, builtins aren't overridden.
- `@mutate`, `@borrow`, and `@fresh`: there are definitely some cases where we can elide copying that GNU-R can't (at least in the example I posted on Slack earlier). But there may also be cases where GNU-R's runtime reference counting can elide copying but we can't, and especially if those cases are in hot code like loops, it would be very expensive to get rid of GNU-R's runtime reference counting. There may be cases where we can elide refcount increments/decrements, but I predict it would be tricky and have negligable performance gain (a non-negligable gain is if we can reuse a large SEXP that GNU-R can't).
- I think we should incorporate e-graphs or some other system, to rewrite expressions into equivalent "normalized" ones, and merge common expressions. I've been thinking about this analysis in Cranelift (https://github.com/bytecodealliance/rfcs/blob/main/accepted/cranelift-egraph.md) that I think we can implement, and maybe extend to cover environment elision and other optimizations.
- Specializing builtins for specific argument types is very useful, but the types don't need to be very complicated. e.g. we want `==` on non-NA scalars to return a boolean, we want `` `[`(<int vector>, <int scalar>) `` and `` `[[`(<int vector>, <int scalar>) `` to return an integer scalar, ...
- We also want to specialize for loops and simple ranges.
  - We might not need true function types, but instead some other system of speculating that a particular variable, when applied with arguments of particular types, returns a value of a particular type.
- Type speculations are also very useful, and we'll need a lot of them to disprove reflection and simplify the IR. Exact value dispatch and speculations may also be useful although less so.
- There are some optimizations that I assumed, that are trivial in the specific case but I don't really know if we'll implement them in the general case. e.g. converting `as.integer(lapply(â€¦))` into `sapply(â€¦)`.
- We may be able to ignore promises in many cases. For instance, I assume most functions don't take side-effecting promise arguments. We could do a "transactional force" of every promise at the callsite of our compiled functions, falling back to a baseline invocation right before we'd do a potentially side-effecting operation, but if the transactional forces succeed, then we can call a compiled version of our code that has no promise arguments. Similarly, we can do transactional forces when loading variables in compiled code: immediately force, and if the variable has a potential side-effect, we stop and deoptimize right before it (if we can hoist external loads to the start of the function this would be even better). Lastly, we'd insert implicitly promise-wrap arguments to GNU-R calls. Then there would be no promise type in compiled closures at all (we still have to deal with arbitrary code from function calls, and deoptimizations from loads, but it would be much easier).
